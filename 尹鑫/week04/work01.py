import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, \
    recall_score
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
import chardet

# è¯»å–æ–‡ä»¶çš„å‰å‡ è¡Œæ¥æ£€æµ‹ç¼–ç 
with open("archive/IMDB Dataset.csv", 'rb') as f:
    result = chardet.detect(f.read())

print(result)

# ==========================
# æ•°æ®åŠ è½½ä¸éªŒè¯ï¼ˆç²¾ç®€ç‰ˆï¼‰
# ==========================
print("æ­£åœ¨åŠ è½½æ•°æ®...")
dataset_df = pd.read_csv("archive/IMDB Dataset.csv", header=None, encoding='utf-8')

# åŸºç¡€éªŒè¯
if dataset_df.isnull().any().any():
    print("âš ï¸  æ£€æµ‹åˆ°ç¼ºå¤±å€¼ï¼Œå·²è‡ªåŠ¨æ¸…ç†")
    dataset_df.dropna(inplace=True)

# é™åˆ¶æ ·æœ¬é‡ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
subset_df = dataset_df.head(500) if len(dataset_df) >= 500 else dataset_df
texts = subset_df[0].astype(str).tolist()
labels_raw = subset_df[1].tolist()

# æ ‡ç­¾ç¼–ç ï¼ˆä¿ç•™ç¼–ç å™¨ç”¨äºåç»­æ˜ å°„ï¼‰
lbl = LabelEncoder()
labels = lbl.fit_transform(labels_raw)
num_classes = len(np.unique(labels))
print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ | æ ·æœ¬æ•°: {len(texts)} | ç±»åˆ«æ•°: {num_classes}")

# ==========================
# æ•°æ®åˆ†å‰²
# ==========================
x_train, x_test, y_train, y_test = train_test_split(
    texts, labels, test_size=0.2, stratify=labels, random_state=42
)
print(f"ğŸ“Š è®­ç»ƒé›†: {len(x_train)} | æµ‹è¯•é›†: {len(x_test)}")

# ==========================
# æ¨¡å‹ä¸åˆ†è¯å™¨
# ==========================
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=num_classes)

# ç¼–ç 
train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=64)
test_encodings = tokenizer(x_test, truncation=True, padding=True, max_length=64)

train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': y_train
})
test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': y_test
})


# ==========================
# å¢å¼ºç‰ˆè¯„ä¼°æŒ‡æ ‡å‡½æ•°
# ==========================
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)

    # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
    acc = accuracy_score(labels, preds)
    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)
    f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)
    precision = precision_score(labels, preds, average='macro', zero_division=0)
    recall = recall_score(labels, preds, average='macro', zero_division=0)

    return {
        'accuracy': acc,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'precision_macro': precision,
        'recall_macro': recall
    }


# ==========================
# è®­ç»ƒé…ç½®ï¼ˆä¼˜åŒ–ï¼‰
# ==========================
os.makedirs('./results', exist_ok=True)
os.makedirs('./logs', exist_ok=True)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,  # è¯„ä¼°æ—¶å¢å¤§batchæå‡é€Ÿåº¦
    warmup_steps=100,  # å°æ•°æ®é›†å‡å°‘warmup
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",  # ä»¥F1ä½œä¸ºæœ€ä½³æ¨¡å‹é€‰æ‹©æ ‡å‡†
    greater_is_better=True,
    report_to="none"  # é¿å…wandbç­‰é¢å¤–æ—¥å¿—
)

# ==========================
# è®­ç»ƒ
# ==========================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)

print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
trainer.train()
print("âœ… è®­ç»ƒå®Œæˆï¼")

# ==========================
# ã€æ ¸å¿ƒå¢å¼ºã€‘å…¨é¢æ¨¡å‹è¯„ä¼°
# ==========================
print("\nğŸ” æ­£åœ¨è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
eval_results = trainer.evaluate()
print("\nğŸ“Œ æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
for key, value in eval_results.items():
    if 'loss' not in key:
        print(f"  {key}: {value:.4f}")

# è·å–é¢„æµ‹ç»“æœï¼ˆç”¨äºè¯¦ç»†åˆ†æï¼‰
predictions = trainer.predict(test_dataset)
preds = np.argmax(predictions.predictions, axis=-1)
true_labels = predictions.label_ids

# 1. è¯¦ç»†åˆ†ç±»æŠ¥å‘Šï¼ˆå«åŸå§‹æ ‡ç­¾åï¼‰
print("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
target_names = [str(cls) for cls in lbl.classes_]
report_dict = classification_report(
    true_labels, preds,
    target_names=target_names,
    output_dict=True,
    zero_division=0
)
print(classification_report(true_labels, preds, target_names=target_names, zero_division=0))

# ä¿å­˜æŠ¥å‘Šåˆ°JSON
with open('./results/classification_report.json', 'w', encoding='utf-8') as f:
    json.dump(report_dict, f, ensure_ascii=False, indent=2)
print("âœ… åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜è‡³: ./results/classification_report.json")

# 2. æ··æ·†çŸ©é˜µå¯è§†åŒ–
plt.figure(figsize=(10, 8))
cm = confusion_matrix(true_labels, preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names)
plt.title('Confusion Matrix', fontsize=16)
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('./results/confusion_matrix.png', dpi=150)
print("âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: ./results/confusion_matrix.png")
plt.close()

# 3. éšæœºå±•ç¤º5ä¸ªé¢„æµ‹ç¤ºä¾‹ï¼ˆå«æ­£ç¡®/é”™è¯¯æ ‡è¯†ï¼‰
print("\nğŸ” é¢„æµ‹ç¤ºä¾‹å±•ç¤ºï¼ˆéšæœº5æ¡ï¼‰:")
np.random.seed(42)
indices = np.random.choice(len(x_test), min(5, len(x_test)), replace=False)
for i in indices:
    text = x_test[i][:50] + "..." if len(x_test[i]) > 50 else x_test[i]
    true_cls = lbl.inverse_transform([true_labels[i]])[0]
    pred_cls = lbl.inverse_transform([preds[i]])[0]
    status = "âœ…" if true_labels[i] == preds[i] else "âŒ"
    print(f"{status} æ–‡æœ¬: {text}")
    print(f"   çœŸå®æ ‡ç­¾: {true_cls} | é¢„æµ‹æ ‡ç­¾: {pred_cls}\n")

# 4. ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå«tokenizerï¼‰
print("\nğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ä½³æ¨¡å‹ä¸åˆ†è¯å™¨...")
trainer.save_model('./results/best_model')
tokenizer.save_pretrained('./results/best_model')
print("âœ… æ¨¡å‹ä¸åˆ†è¯å™¨å·²ä¿å­˜è‡³: ./results/best_model")

# 5. ç”Ÿæˆè¯„ä¼°æ‘˜è¦
summary = {
    "total_samples": len(texts),
    "train_size": len(x_train),
    "test_size": len(x_test),
    "num_classes": num_classes,
    "final_accuracy": float(eval_results['eval_accuracy']),
    "final_f1_macro": float(eval_results['eval_f1_macro']),
    "model_path": "./results/best_model"
}
with open('./results/evaluation_summary.json', 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)
print("âœ… è¯„ä¼°æ‘˜è¦å·²ä¿å­˜è‡³: ./results/evaluation_summary.json")

print("\nğŸ‰ æ‰€æœ‰è¯„ä¼°ä»»åŠ¡å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ ./results ç›®å½•")
